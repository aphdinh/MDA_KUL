{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge files\n",
    "def merge_csv_files(directory_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Merges all CSV files in a directory into a single pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    directory_path (str): The directory path containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    merged_df (pandas.DataFrame): The merged pandas DataFrame of all CSV files in the directory.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "\n",
    "    # loop through each file in the directory\n",
    "    for file in os.listdir(directory_path):\n",
    "        # check if the file is a CSV file\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory_path, file)\n",
    "            if directory_path.endswith(\"dataverse_file\"):\n",
    "                df = pd.read_csv(file_path, delimiter=\",\")\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, delimiter=\";\")\n",
    "            dfs.append(df)\n",
    "\n",
    "    # concatenate all dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get historical air quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get air quality data for Leuven\n",
    "# Period: Jan 1, 2022 to Dec 31, 2022\n",
    "api_key = \"ec722f11f234fb9a316e2580e6e2019e\"\n",
    "lat = 50.88\n",
    "lon = 4.7\n",
    "start_date = \"1640995200\"\n",
    "end_date = \"1672531200\"\n",
    "\n",
    "url = f\"http://api.openweathermap.org/data/2.5/air_pollution/history?lat={lat}&lon={lon}&start={start_date}&end={end_date}&appid={api_key}\"\n",
    "# url = f\"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}\"\n",
    "# url = f\"http://api.openweathermap.org/data/2.5/air_pollution/forecast?lat={lat}&lon={lon}&appid={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>co</th>\n",
       "      <th>no</th>\n",
       "      <th>no2</th>\n",
       "      <th>o3</th>\n",
       "      <th>so2</th>\n",
       "      <th>pm2_5</th>\n",
       "      <th>pm10</th>\n",
       "      <th>nh3</th>\n",
       "      <th>aqi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>253.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.83</td>\n",
       "      <td>39.34</td>\n",
       "      <td>1.82</td>\n",
       "      <td>3.68</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>250.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.05</td>\n",
       "      <td>38.62</td>\n",
       "      <td>1.86</td>\n",
       "      <td>3.69</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>247.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.11</td>\n",
       "      <td>38.27</td>\n",
       "      <td>1.71</td>\n",
       "      <td>3.44</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "      <td>243.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.17</td>\n",
       "      <td>38.62</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>257.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>37.19</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.02</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-01-01 05:00:00</td>\n",
       "      <td>290.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.11</td>\n",
       "      <td>32.54</td>\n",
       "      <td>0.95</td>\n",
       "      <td>4.49</td>\n",
       "      <td>5.46</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-01-01 06:00:00</td>\n",
       "      <td>330.45</td>\n",
       "      <td>0.02</td>\n",
       "      <td>15.42</td>\n",
       "      <td>26.82</td>\n",
       "      <td>1.18</td>\n",
       "      <td>6.74</td>\n",
       "      <td>7.79</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-01-01 07:00:00</td>\n",
       "      <td>337.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>18.68</td>\n",
       "      <td>23.96</td>\n",
       "      <td>1.48</td>\n",
       "      <td>7.13</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-01-01 08:00:00</td>\n",
       "      <td>307.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>18.16</td>\n",
       "      <td>25.03</td>\n",
       "      <td>1.64</td>\n",
       "      <td>5.74</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-01-01 09:00:00</td>\n",
       "      <td>273.70</td>\n",
       "      <td>0.09</td>\n",
       "      <td>15.42</td>\n",
       "      <td>28.61</td>\n",
       "      <td>1.59</td>\n",
       "      <td>4.23</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dt      co    no    no2     o3   so2  pm2_5  pm10   nh3  \\\n",
       "0 2022-01-01 00:00:00  253.68  0.00   8.83  39.34  1.82   3.68  5.20  0.79   \n",
       "1 2022-01-01 01:00:00  250.34  0.00   8.05  38.62  1.86   3.69  5.27  0.79   \n",
       "2 2022-01-01 02:00:00  247.00  0.00   7.11  38.27  1.71   3.44  4.99  0.78   \n",
       "3 2022-01-01 03:00:00  243.66  0.00   6.17  38.62  1.33   2.93  4.26  0.72   \n",
       "4 2022-01-01 04:00:00  257.02  0.00   6.85  37.19  1.03   3.02  4.07  0.68   \n",
       "5 2022-01-01 05:00:00  290.39  0.00  10.11  32.54  0.95   4.49  5.46  0.69   \n",
       "6 2022-01-01 06:00:00  330.45  0.02  15.42  26.82  1.18   6.74  7.79  0.73   \n",
       "7 2022-01-01 07:00:00  337.12  0.04  18.68  23.96  1.48   7.13  8.26  0.76   \n",
       "8 2022-01-01 08:00:00  307.08  0.04  18.16  25.03  1.64   5.74  6.81  0.71   \n",
       "9 2022-01-01 09:00:00  273.70  0.09  15.42  28.61  1.59   4.23  5.20  0.66   \n",
       "\n",
       "   aqi  \n",
       "0    1  \n",
       "1    1  \n",
       "2    1  \n",
       "3    1  \n",
       "4    1  \n",
       "5    1  \n",
       "6    1  \n",
       "7    1  \n",
       "8    1  \n",
       "9    1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Historical\n",
    "df = pd.DataFrame(data[\"list\"])\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"], unit=\"s\")\n",
    "components_df = pd.json_normalize(df[\"components\"])\n",
    "aqi_df = pd.json_normalize(df[\"main\"])\n",
    "df = pd.concat([df, components_df, aqi_df], axis=1)\n",
    "df.drop([\"components\", \"main\"], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process air quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_data = df\n",
    "\n",
    "# Format time stamp\n",
    "air_quality_data[\"dt\"] = pd.to_datetime(air_quality_data[\"dt\"])\n",
    "air_quality_data[\"date\"] = air_quality_data[\"dt\"].dt.date\n",
    "air_quality_data[\"hour\"] = air_quality_data[\"dt\"].dt.hour\n",
    "air_quality_data[\"month\"] = air_quality_data[\"dt\"].dt.month\n",
    "air_quality_data[\"weekday\"] = air_quality_data[\"dt\"].dt.strftime(\"%a\")\n",
    "air_quality_data.to_csv(\"../data/processed_air_quality_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get historical traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://telraam-api.net/v1/reports/traffic\"\n",
    "headers = {\"X-Api-Key\": \"Z9Qoyuy9Yf99nf2I0Myig4t0ftuUmY81ahBIVZH4\"}\n",
    "\n",
    "start_date = pd.to_datetime(\"2022-01-01\")\n",
    "end_date = pd.to_datetime(\"2023-01-01\")\n",
    "\n",
    "# define the duration for each loop (3 months)\n",
    "loop_duration = pd.DateOffset(months=3)\n",
    "\n",
    "# define the list of sensor IDs\n",
    "counter_ids = [\n",
    "    \"9000000627\",\n",
    "    \"347690\",\n",
    "    \"9000000674\",\n",
    "    \"9000000773\",\n",
    "    \"347931\",\n",
    "    \"347860\",\n",
    "    \"9000000764\",\n",
    "    \"9000000672\",\n",
    "    \"347948\",\n",
    "    \"9000001547\",\n",
    "    \"347365\",\n",
    "    \"349054\",\n",
    "    \"9000000681\",\n",
    "]\n",
    "\n",
    "full_data = pd.DataFrame()\n",
    "\n",
    "# loop through the counter IDs\n",
    "for counter_id in counter_ids:\n",
    "    # retrieve data for each 3-month period\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        loop_start_date = current_date\n",
    "        loop_end_date = loop_start_date + loop_duration\n",
    "\n",
    "        body = {\n",
    "            \"id\": counter_id,\n",
    "            \"time_start\": loop_start_date.strftime(\"%Y-%m-%d %H:%M:%SZ\"),\n",
    "            \"time_end\": loop_end_date.strftime(\"%Y-%m-%d %H:%M:%SZ\"),\n",
    "            \"level\": \"segments\",\n",
    "            \"format\": \"per-hour\",\n",
    "        }\n",
    "        payload = str(body)\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        json_data = response.json()\n",
    "        loop_data = pd.DataFrame(json_data[\"report\"])\n",
    "        full_data = pd.concat([full_data, loop_data])\n",
    "\n",
    "        # update the current date for the next loop\n",
    "        current_date += loop_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = {\n",
    "    \"9000000627\": \"redingenhof\",\n",
    "    \"347690\": \"Kapucijnenvoer\",\n",
    "    \"9000000674\": \"Tiensestraat\",\n",
    "    \"9000000773\": \"Bondgenotenlaan\",\n",
    "    \"347931\": \"Vital Decostersstraat\",\n",
    "    \"347860\": \"BROUWERSSTRAAT\",\n",
    "    \"9000000764\": \"Fonteinstraat 137 b 301\",\n",
    "    \"9000000672\": \"Petermannenstraat\",\n",
    "    \"347948\": \"Ridderstraat\",\n",
    "    \"9000001547\": \"Jan Pieter Minckelersstraat\",\n",
    "    \"347365\": \"Dekenstraat\",\n",
    "    \"349054\": \"Pleinstraat\",\n",
    "    \"9000000681\": \"Bierbeekstraat\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"location\"] = full_data.segment_id.astype(\"str\").map(location)\n",
    "full_data.to_csv(\"../data/traffic_leuven_2022.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get historical weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_hourly_weather(url):\n",
    "    resp = requests.get(url)\n",
    "    data = resp.json()\n",
    "    df = pd.DataFrame(data[\"hourly\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive-api.open-meteo.com/v1/archive?latitude=50.88&longitude=4.70&start_date=2022-01-01&end_date=2022-12-31&timezone=Europe%2FBerlin&hourly=temperature_2m,relativehumidity_2m,dewpoint_2m,apparent_temperature,pressure_msl,surface_pressure,precipitation,snowfall,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,shortwave_radiation,direct_radiation,diffuse_radiation,direct_normal_irradiance,windspeed_10m,winddirection_10m,windgusts_10m\"\n",
    "weather_data = get_forecast_hourly_weather(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process historical weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename the column\n",
    "weather_data.columns = [re.sub(\" \\(.*\\)\", \"\", col) for col in weather_data.columns]\n",
    "\n",
    "# Format time stamp\n",
    "weather_data[\"time\"] = pd.to_datetime(weather_data[\"time\"])\n",
    "weather_data[\"date\"] = weather_data[\"time\"].dt.date\n",
    "weather_data[\"hour\"] = weather_data[\"time\"].dt.hour\n",
    "weather_data[\"month\"] = weather_data[\"time\"].dt.month\n",
    "weather_data[\"weekday\"] = weather_data[\"time\"].dt.strftime(\"%a\")\n",
    "\n",
    "weather_data.to_csv(\"../data/processed_weather_data_leuven.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process file 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file40 = merge_csv_files(\"../data/file40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we would delete unit column because of same values\n",
    "# drop all _unit columns\n",
    "cols_to_drop = [col for col in file40.columns if col.endswith(\"unit\")]\n",
    "file40.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# rename columns\n",
    "file40.rename(\n",
    "    columns={\"description\": \"location\", \"#object_id\": \"object_id\"}, inplace=True\n",
    ")\n",
    "\n",
    "# Convert the 'result_timestamp' column to a datetime data type\n",
    "file40[\"result_timestamp\"] = pd.to_datetime(file40[\"result_timestamp\"])\n",
    "file40[\"date\"] = file40[\"result_timestamp\"].dt.date\n",
    "file40[\"hour\"] = file40[\"result_timestamp\"].dt.hour\n",
    "file40[\"month\"] = file40[\"result_timestamp\"].dt.month\n",
    "file40[\"weekday\"] = file40[\"result_timestamp\"].dt.strftime(\"%a\")\n",
    "\n",
    "file40.to_csv(\"../data/processed_file40_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process file 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge files\n",
    "file41 = merge_csv_files(\"../data/file41\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unncessary cols\n",
    "cols_to_drop = [\n",
    "    \"noise_event_laeq_model_id_unit\",\n",
    "    \"noise_event_laeq_model_id\",\n",
    "    \"noise_event_laeq_primary_detected_certainty_unit\",\n",
    "    \"noise_event_laeq_primary_detected_class_unit\",\n",
    "]\n",
    "\n",
    "file41.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# rename cols\n",
    "file41.columns = [\n",
    "    \"object_id\",\n",
    "    \"location\",\n",
    "    \"result_timestamp\",\n",
    "    \"noise_event_certainty\",\n",
    "    \"noise_event\",\n",
    "]\n",
    "\n",
    "# remove the noise_event that are unsupported\n",
    "file41 = file41.loc[file41.noise_event != \"Unsupported\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from timestamp\n",
    "file41[\"result_timestamp\"] = pd.to_datetime(file41[\"result_timestamp\"])\n",
    "file41[\"time\"] = file41[\"result_timestamp\"].dt.time\n",
    "file41[\"date\"] = file41[\"result_timestamp\"].dt.date\n",
    "file41[\"hour\"] = file41[\"result_timestamp\"].dt.hour\n",
    "file41[\"month\"] = file41[\"result_timestamp\"].dt.month\n",
    "file41[\"weekday\"] = file41[\"result_timestamp\"].dt.strftime(\"%a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only adding datapoints with certanity of 85%+\n",
    "file41 = pd.DataFrame(file41.loc[file41[\"noise_event_certainty\"] > 85].reset_index())\n",
    "# Dropping undefined columns\n",
    "file41 = file41.dropna(subset=[\"noise_event\"])\n",
    "# Pivot table to transform into counting hourly types of noise events\n",
    "file41_piv = (\n",
    "    pd.pivot_table(\n",
    "        file41,\n",
    "        index=[\"object_id\", \"date\", \"hour\", \"month\", \"weekday\"],\n",
    "        columns=[\"noise_event\"],\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "    .xs(\"location\", level=0, axis=1)\n",
    "    .reset_index()\n",
    ")\n",
    "file41_piv.fillna(0, inplace=True)\n",
    "\n",
    "file41_piv.to_csv(\"../data/processed_file41_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process file 42 - full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder containing the full data\n",
    "main_folder = \"s3://teamchadmda/export_42_full/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of months and their numerical codes used in the data structure\n",
    "months = [\n",
    "    'Jan', 'Feb', 'March', 'April', 'May', 'June',\n",
    "    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
    "]\n",
    "num_codes = range(42,53)\n",
    "num_codes = [str(num) for num in num_codes]\n",
    "month_codes = {month: code for month,code in zip(months,num_codes)}\n",
    "month_codes['Feb'] = '42'\n",
    "\n",
    "#dictionary of location codes\n",
    "object_id_dict = {\n",
    "    255439: \"MP 01: Naamsestraat 35 Maxim\",\n",
    "    255440: \"MP 02: Naamsestraat 57 Xior\",\n",
    "    255441: \"MP 03: Naamsestraat 62 Taste\",\n",
    "    303910: \"MP 04: His & Hears\",\n",
    "    255442: \"MP 05: Calvariekapel KU Leuven\",\n",
    "    255443: \"MP 06: Parkstraat 2 La Filosovia\",\n",
    "    255444: \"MP 07: Naamsestraat 81\",\n",
    "    280324: \"MP08bis - Vrijthof\",\n",
    "}\n",
    "\n",
    "#File name list, the stuff that comes after csv_results_[monthcode]\n",
    "file_name_exts = [\"_255439_mp-01-naamsestraat-35-maxim.csv\",\n",
    "                 \"_255440_mp-02-naamsestraat-57-xior.csv\",\n",
    "                 \"_255441_mp-03-naamsestraat-62-taste.csv\",\n",
    "                 \"_255442_mp-05-calvariekapel-ku-leuven.csv\",\n",
    "                 \"_255443_mp-06-parkstraat-2-la-filosovia.csv\",\n",
    "                 \"_255444_mp-07-naamsestraat-81.csv\",\n",
    "                 \"_255445_mp-08-kiosk-stadspark.csv\",\n",
    "                 \"_280324_mp08bis---vrijthof.csv\",\n",
    "                 \"_303910_mp-04-his-hears.csv\",]\n",
    "\n",
    "#function to give the list of files for a month's folder\n",
    "def filelist(monthstring):\n",
    "    filelist = []\n",
    "    for file in file_name_exts:\n",
    "        filelist.append(main_folder+monthstring+\"/csv_results_\"+month_codes[monthstring]+file)\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://teamchadmda/export_42_full/March/csv_results_44_255439_mp-01-naamsestraat-35-maxim.csv',\n",
       " 's3://teamchadmda/export_42_full/March/csv_results_44_255440_mp-02-naamsestraat-57-xior.csv',\n",
       " 's3://teamchadmda/export_42_full/March/csv_results_44_255441_mp-03-naamsestraat-62-taste.csv',\n",
       " 's3://teamchadmda/export_42_full/March/csv_results_44_255442_mp-05-calvariekapel-ku-leuven.csv',\n",
       " 's3://teamchadmda/export_42_full/March/csv_results_44_255443_mp-06-parkstraat-2-la-filosovia.csv',\n",
       " 's3://teamchadmda/export_42_full/March/csv_results_44_255444_mp-07-naamsestraat-81.csv',\n",
       " 's3://teamchadmda/export_42_full/March/csv_results_44_255445_mp-08-kiosk-stadspark.csv',\n",
       " 's3://teamchadmda/export_42_full/March/csv_results_44_280324_mp08bis---vrijthof.csv',\n",
       " 's3://teamchadmda/export_42_full/March/csv_results_44_303910_mp-04-his-hears.csv']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist('March')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process a given file in the file42 directory\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    df = pd.read_csv(file_path, delimiter=\";\")\n",
    "    \n",
    "    # convert 'result_timestamp' to datetime format\n",
    "    df[\"result_timestamp\"] = pd.to_datetime(\n",
    "        df[\"result_timestamp\"], format=\"%d/%m/%Y %H:%M:%S.%f\"\n",
    "    )\n",
    "    \n",
    "    # set 'result_timestamp' as the index\n",
    "    df.set_index(\"result_timestamp\", inplace=True)\n",
    "    \n",
    "    # resample to hourly frequency and calculate the mean\n",
    "    df_resampled = df.resample(\"H\").mean()\n",
    "    \n",
    "    # reset index and add additional columns\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "    df_resampled[\"date\"] = df_resampled[\"result_timestamp\"].dt.date\n",
    "    df_resampled[\"hour\"] = df_resampled[\"result_timestamp\"].dt.hour\n",
    "    df_resampled[\"weekday\"] = df_resampled[\"result_timestamp\"].dt.strftime(\"%a\")\n",
    "    df_resampled[\"month\"] = df_resampled[\"result_timestamp\"].dt.month\n",
    "    \n",
    "    # drop rows with NaN values in 'lamax' column\n",
    "    df_resampled.dropna(subset=[\"lamax\"], inplace=True)\n",
    "    \n",
    "    # rename column and convert 'object_id' to int\n",
    "    df_resampled.rename(columns={\"#object_id\": \"object_id\"}, inplace=True)\n",
    "    df_resampled[\"object_id\"] = df_resampled[\"object_id\"].astype(int)\n",
    "    \n",
    "    # ddd location column using 'object_id_dict'\n",
    "    df_resampled[\"location\"] = df_resampled[\"object_id\"].map(object_id_dict)\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that goes through entire filesystem, processes files, and gathers it all together\n",
    "\n",
    "def gather_file_42():\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    \n",
    "    for month in months:\n",
    "        file_list = filelist(month)\n",
    "        \n",
    "        for file in file_list:\n",
    "            df_resampled = process_file(file)\n",
    "            # concatenate\n",
    "            concatenated_df = pd.concat([concatenated_df, df_resampled])\n",
    "            concatenated_df = concatenated_df[\n",
    "                [\n",
    "                    \"result_timestamp\",\n",
    "                    \"object_id\",\n",
    "                    \"lamax\",\n",
    "                    \"laeq\",\n",
    "                    \"lceq\",\n",
    "                    \"lcpeak\",\n",
    "                    \"date\",\n",
    "                    \"hour\",\n",
    "                    \"weekday\",\n",
    "                    \"month\",\n",
    "                    \"location\",\n",
    "                ]\n",
    "            ]\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file42 = gather_file_42()\n",
    "file42.to_csv(\"../data/processed_file42.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
