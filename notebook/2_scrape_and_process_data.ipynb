{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge files\n",
    "def merge_csv_files(directory_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Merges all CSV files in a directory into a single pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    directory_path (str): The directory path containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    merged_df (pandas.DataFrame): The merged pandas DataFrame of all CSV files in the directory.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "\n",
    "    # loop through each file in the directory\n",
    "    for file in os.listdir(directory_path):\n",
    "        # check if the file is a CSV file\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory_path, file)\n",
    "            if directory_path.endswith(\"dataverse_file\"):\n",
    "                df = pd.read_csv(file_path, delimiter=\",\")\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, delimiter=\";\")\n",
    "            dfs.append(df)\n",
    "\n",
    "    # concatenate all dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get historical air quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get air quality data for Leuven\n",
    "# Period: Jan 1, 2022 to Dec 31, 2022\n",
    "api_key = \"ec722f11f234fb9a316e2580e6e2019e\"\n",
    "lat = 50.88\n",
    "lon = 4.7\n",
    "start_date = \"1640995200\"\n",
    "end_date = \"1672531200\"\n",
    "\n",
    "url = f\"http://api.openweathermap.org/data/2.5/air_pollution/history?lat={lat}&lon={lon}&start={start_date}&end={end_date}&appid={api_key}\"\n",
    "# url = f\"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}\"\n",
    "# url = f\"http://api.openweathermap.org/data/2.5/air_pollution/forecast?lat={lat}&lon={lon}&appid={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical\n",
    "df = pd.DataFrame(data[\"list\"])\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"], unit=\"s\")\n",
    "components_df = pd.json_normalize(df[\"components\"])\n",
    "aqi_df = pd.json_normalize(df[\"main\"])\n",
    "df = pd.concat([df, components_df, aqi_df], axis=1)\n",
    "df.drop([\"components\", \"main\"], axis=1, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process air quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_data = df\n",
    "\n",
    "# Format time stamp\n",
    "air_quality_data[\"dt\"] = pd.to_datetime(air_quality_data[\"dt\"])\n",
    "air_quality_data[\"date\"] = air_quality_data[\"dt\"].dt.date\n",
    "air_quality_data[\"hour\"] = air_quality_data[\"dt\"].dt.hour\n",
    "air_quality_data[\"month\"] = air_quality_data[\"dt\"].dt.month\n",
    "air_quality_data[\"weekday\"] = air_quality_data[\"dt\"].dt.strftime(\"%a\")\n",
    "\n",
    "air_quality_data.to_csv(\"../data/processed_air_quality_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get historical traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://telraam-api.net/v1/reports/traffic\"\n",
    "headers = {\"X-Api-Key\": \"Z9Qoyuy9Yf99nf2I0Myig4t0ftuUmY81ahBIVZH4\"}\n",
    "\n",
    "start_date = pd.to_datetime(\"2022-01-01\")\n",
    "end_date = pd.to_datetime(\"2023-01-01\")\n",
    "\n",
    "# define the duration for each loop (3 months)\n",
    "loop_duration = pd.DateOffset(months=3)\n",
    "\n",
    "# define the list of sensor IDs\n",
    "counter_ids = [\n",
    "    \"9000000627\",\n",
    "    \"347690\",\n",
    "    \"9000000674\",\n",
    "    \"9000000773\",\n",
    "    \"347931\",\n",
    "    \"347860\",\n",
    "    \"9000000764\",\n",
    "    \"9000000672\",\n",
    "    \"347948\",\n",
    "    \"9000001547\",\n",
    "    \"347365\",\n",
    "    \"349054\",\n",
    "    \"9000000681\",\n",
    "]\n",
    "\n",
    "full_data = pd.DataFrame()\n",
    "\n",
    "# loop through the counter IDs\n",
    "for counter_id in counter_ids:\n",
    "    # retrieve data for each 3-month period\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        loop_start_date = current_date\n",
    "        loop_end_date = loop_start_date + loop_duration\n",
    "\n",
    "        body = {\n",
    "            \"id\": counter_id,\n",
    "            \"time_start\": loop_start_date.strftime(\"%Y-%m-%d %H:%M:%SZ\"),\n",
    "            \"time_end\": loop_end_date.strftime(\"%Y-%m-%d %H:%M:%SZ\"),\n",
    "            \"level\": \"segments\",\n",
    "            \"format\": \"per-hour\",\n",
    "        }\n",
    "        payload = str(body)\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        json_data = response.json()\n",
    "        loop_data = pd.DataFrame(json_data[\"report\"])\n",
    "        full_data = pd.concat([full_data, loop_data])\n",
    "\n",
    "        # update the current date for the next loop\n",
    "        current_date += loop_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = {\n",
    "    \"9000000627\": \"redingenhof\",\n",
    "    \"347690\": \"Kapucijnenvoer\",\n",
    "    \"9000000674\": \"Tiensestraat\",\n",
    "    \"9000000773\": \"Bondgenotenlaan\",\n",
    "    \"347931\": \"Vital Decostersstraat\",\n",
    "    \"347860\": \"BROUWERSSTRAAT\",\n",
    "    \"9000000764\": \"Fonteinstraat 137 b 301\",\n",
    "    \"9000000672\": \"Petermannenstraat\",\n",
    "    \"347948\": \"Ridderstraat\",\n",
    "    \"9000001547\": \"Jan Pieter Minckelersstraat\",\n",
    "    \"347365\": \"Dekenstraat\",\n",
    "    \"349054\": \"Pleinstraat\",\n",
    "    \"9000000681\": \"Bierbeekstraat\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"location\"] = full_data.segment_id.astype(\"str\").map(location)\n",
    "full_data.to_csv(\"../data/traffic_leuven_2022.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get historical weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_hourly_weather(url):\n",
    "    resp = requests.get(url)\n",
    "    data = resp.json()\n",
    "    df = pd.DataFrame(data[\"hourly\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive-api.open-meteo.com/v1/archive?latitude=50.88&longitude=4.70&start_date=2022-01-01&end_date=2022-12-31&timezone=Europe%2FBerlin&hourly=temperature_2m,relativehumidity_2m,dewpoint_2m,apparent_temperature,pressure_msl,surface_pressure,precipitation,snowfall,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,shortwave_radiation,direct_radiation,diffuse_radiation,direct_normal_irradiance,windspeed_10m,winddirection_10m,windgusts_10m\"\n",
    "weather_data = get_forecast_hourly_weather(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process historical weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename the column\n",
    "weather_data.columns = [re.sub(\" \\(.*\\)\", \"\", col) for col in weather_data.columns]\n",
    "\n",
    "# Format time stamp\n",
    "weather_data[\"time\"] = pd.to_datetime(weather_data[\"time\"])\n",
    "weather_data[\"date\"] = weather_data[\"time\"].dt.date\n",
    "weather_data[\"hour\"] = weather_data[\"time\"].dt.hour\n",
    "weather_data[\"month\"] = weather_data[\"time\"].dt.month\n",
    "weather_data[\"weekday\"] = weather_data[\"time\"].dt.strftime(\"%a\")\n",
    "\n",
    "weather_data.to_csv(\"../data/processed_weather_data_leuven.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data[\"temperature_2m\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process file 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file40 = merge_csv_files(\"../data/file40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we would delete unit column because of same values\n",
    "# drop all _unit columns\n",
    "cols_to_drop = [col for col in file40.columns if col.endswith(\"unit\")]\n",
    "file40.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# rename columns\n",
    "file40.rename(\n",
    "    columns={\"description\": \"location\", \"#object_id\": \"object_id\"}, inplace=True\n",
    ")\n",
    "\n",
    "# Convert the 'result_timestamp' column to a datetime data type\n",
    "file40[\"result_timestamp\"] = pd.to_datetime(file40[\"result_timestamp\"])\n",
    "file40[\"date\"] = file40[\"result_timestamp\"].dt.date\n",
    "file40[\"hour\"] = file40[\"result_timestamp\"].dt.hour\n",
    "file40[\"month\"] = file40[\"result_timestamp\"].dt.month\n",
    "file40[\"weekday\"] = file40[\"result_timestamp\"].dt.strftime(\"%a\")\n",
    "\n",
    "file40.to_csv(\"../data/processed_file40_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process file 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge files\n",
    "file41 = merge_csv_files(\"../data/file41\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unncessary cols\n",
    "cols_to_drop = [\n",
    "    \"noise_event_laeq_model_id_unit\",\n",
    "    \"noise_event_laeq_model_id\",\n",
    "    \"noise_event_laeq_primary_detected_certainty_unit\",\n",
    "    \"noise_event_laeq_primary_detected_class_unit\",\n",
    "]\n",
    "\n",
    "file41.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# rename cols\n",
    "file41.columns = [\n",
    "    \"object_id\",\n",
    "    \"location\",\n",
    "    \"result_timestamp\",\n",
    "    \"noise_event_certainty\",\n",
    "    \"noise_event\",\n",
    "]\n",
    "\n",
    "# remove the noise_event that are unsupported\n",
    "file41 = file41.loc[file41.noise_event != \"Unsupported\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from timestamp\n",
    "file41[\"result_timestamp\"] = pd.to_datetime(file41[\"result_timestamp\"])\n",
    "file41[\"time\"] = file41[\"result_timestamp\"].dt.time\n",
    "file41[\"date\"] = file41[\"result_timestamp\"].dt.date\n",
    "file41[\"hour\"] = file41[\"result_timestamp\"].dt.hour\n",
    "file41[\"month\"] = file41[\"result_timestamp\"].dt.month\n",
    "file41[\"weekday\"] = file41[\"result_timestamp\"].dt.strftime(\"%a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only adding datapoints with certanity of 85%+\n",
    "file41 = pd.DataFrame(file41.loc[file41[\"noise_event_certainty\"] > 85].reset_index())\n",
    "# Dropping undefined columns\n",
    "file41 = file41.dropna(subset=[\"noise_event\"])\n",
    "# Pivot table to transform into counting hourly types of noise events\n",
    "file41_piv = (\n",
    "    pd.pivot_table(\n",
    "        file41,\n",
    "        index=[\"object_id\", \"date\", \"hour\", \"month\", \"weekday\"],\n",
    "        columns=[\"noise_event\"],\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "    .xs(\"location\", level=0, axis=1)\n",
    "    .reset_index()\n",
    ")\n",
    "file41_piv.fillna(0, inplace=True)\n",
    "\n",
    "file41_piv.to_csv(\"../data/processed_file41_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process file 42 - full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder containing the full data\n",
    "main_folder = \"s3://teamchadmda/export_42_full/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of months and their numerical codes used in the data structure\n",
    "months = [\n",
    "    'Jan', 'Feb', 'March', 'April', 'May', 'June',\n",
    "    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
    "]\n",
    "num_codes = range(42,53)\n",
    "num_codes = [str(num) for num in num_codes]\n",
    "month_codes = {month: code for month,code in zip(months,num_codes)}\n",
    "month_codes['Feb'] = '42'\n",
    "\n",
    "#dictionary of location codes\n",
    "object_id_dict = {\n",
    "    255439: \"MP 01: Naamsestraat 35 Maxim\",\n",
    "    255440: \"MP 02: Naamsestraat 57 Xior\",\n",
    "    255441: \"MP 03: Naamsestraat 62 Taste\",\n",
    "    303910: \"MP 04: His & Hears\",\n",
    "    255442: \"MP 05: Calvariekapel KU Leuven\",\n",
    "    255443: \"MP 06: Parkstraat 2 La Filosovia\",\n",
    "    255444: \"MP 07: Naamsestraat 81\",\n",
    "    280324: \"MP08bis - Vrijthof\",\n",
    "}\n",
    "\n",
    "#File name list, the stuff that comes after csv_results_[monthcode]\n",
    "file_name_exts = [\"_255439_mp-01-naamsestraat-35-maxim.csv\",\n",
    "                 \"_255440_mp-02-naamsestraat-57-xior.csv\",\n",
    "                 \"_255441_mp-03-naamsestraat-62-taste.csv\",\n",
    "                 \"_255442_mp-05-calvariekapel-ku-leuven.csv\",\n",
    "                 \"_255443_mp-06-parkstraat-2-la-filosovia.csv\",\n",
    "                 \"_255444_mp-07-naamsestraat-81.csv\",\n",
    "                 \"_255445_mp-08-kiosk-stadspark.csv\",\n",
    "                 \"_280324_mp08bis---vrijthof.csv\",\n",
    "                 \"_303910_mp-04-his-hears.csv\",]\n",
    "\n",
    "#function to give the list of files for a month's folder\n",
    "def filelist(monthstring):\n",
    "    filelist = []\n",
    "    for file in file_name_exts:\n",
    "        filelist.append(main_folder+monthstring+\"/csv_results_\"+month_codes[monthstring]+file)\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist('March')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process a given file in the file42 directory\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    df = pd.read_csv(file_path, delimiter=\";\")\n",
    "    \n",
    "    # convert 'result_timestamp' to datetime format\n",
    "    df[\"result_timestamp\"] = pd.to_datetime(\n",
    "        df[\"result_timestamp\"], format=\"%d/%m/%Y %H:%M:%S.%f\"\n",
    "    )\n",
    "    \n",
    "    # set 'result_timestamp' as the index\n",
    "    df.set_index(\"result_timestamp\", inplace=True)\n",
    "    \n",
    "    # resample to hourly frequency and calculate the mean\n",
    "    df_resampled = df.resample(\"H\").mean()\n",
    "    \n",
    "    # reset index and add additional columns\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "    df_resampled[\"date\"] = df_resampled[\"result_timestamp\"].dt.date\n",
    "    df_resampled[\"hour\"] = df_resampled[\"result_timestamp\"].dt.hour\n",
    "    df_resampled[\"weekday\"] = df_resampled[\"result_timestamp\"].dt.strftime(\"%a\")\n",
    "    df_resampled[\"month\"] = df_resampled[\"result_timestamp\"].dt.month\n",
    "    \n",
    "    # drop rows with NaN values in 'lamax' column\n",
    "    df_resampled.dropna(subset=[\"lamax\"], inplace=True)\n",
    "    \n",
    "    # rename column and convert 'object_id' to int\n",
    "    df_resampled.rename(columns={\"#object_id\": \"object_id\"}, inplace=True)\n",
    "    df_resampled[\"object_id\"] = df_resampled[\"object_id\"].astype(int)\n",
    "    \n",
    "    # ddd location column using 'object_id_dict'\n",
    "    df_resampled[\"location\"] = df_resampled[\"object_id\"].map(object_id_dict)\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that goes through entire filesystem, processes files, and gathers it all together\n",
    "\n",
    "def gather_file_42():\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    \n",
    "    for month in months:\n",
    "        file_list = filelist(month)\n",
    "        \n",
    "        for file in file_list:\n",
    "            df_resampled = process_file(file)\n",
    "            # concatenate\n",
    "            concatenated_df = pd.concat([concatenated_df, df_resampled])\n",
    "            concatenated_df = concatenated_df[\n",
    "                [\n",
    "                    \"result_timestamp\",\n",
    "                    \"object_id\",\n",
    "                    \"lamax\",\n",
    "                    \"laeq\",\n",
    "                    \"lceq\",\n",
    "                    \"lcpeak\",\n",
    "                    \"date\",\n",
    "                    \"hour\",\n",
    "                    \"weekday\",\n",
    "                    \"month\",\n",
    "                    \"location\",\n",
    "                ]\n",
    "            ]\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file42 = gather_file_42()\n",
    "file42.to_csv(\"../data/processed_file42.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
