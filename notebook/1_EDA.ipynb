{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pytz\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How do meteorological conditions, such as wind speed and precipitation, impact noise levels in different locations of Leuven?\n",
    "\n",
    "2. How do noise levels vary across different times of day and days of the week in Leuven? On which days do we expect higher amount of noise level?\n",
    "\n",
    "3. How does the noise level vary across different times of the year? Are there any particular months or seasons when the noise level is higher or lower?\n",
    "\n",
    "4. Is there any spatial patterns in noise levels within the area? Which location is consistently noisier than others?\n",
    "5. How does the density and location of restaurants, bars and other destinations in the neighborhoods affect noise levels? \n",
    "6. Are there any temporal patterns in noise levels? For example, are there any specific hours of the night when the noise level tends to be higher or lower?\n",
    "7. Can we identify any patterns in noise levels across different times of the day (not just nighttime)? For example, do noise levels tend to be higher during rush hour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files(directory_path,file_list,delim=\";\"):\n",
    "    \"\"\"\n",
    "    Merges all CSV files in a directory into a single pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    directory_path (str): The directory path containing the CSV files.\n",
    "    delim (str): character used for delimiter in CSV files\n",
    "    file_list = list of strings of individual file names\n",
    "\n",
    "    Returns:\n",
    "    merged_df (pandas.DataFrame): The merged pandas DataFrame of all CSV files in the directory.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "\n",
    "    # loop through each file in the directory\n",
    "    for file in file_list:\n",
    "        # check if the file is a CSV file\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = directory_path + file\n",
    "            df = pd.read_csv(file_path, delimiter=delim)\n",
    "            dfs.append(df)\n",
    "\n",
    "    # concatenate all dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise data\n",
    "\n",
    "folder_path = \"s3://teamchadmda\"\n",
    "file_list_40 = [\"csv_results_40_255439_mp-01-naamsestraat-35-maxim.csv\",\n",
    "               \"csv_results_40_255440_mp-02-naamsestraat-57-xior.csv\",\n",
    "               \"csv_results_40_255441_mp-03-naamsestraat-62-taste.csv\",\n",
    "               \"csv_results_40_255442_mp-05-calvariekapel-ku-leuven.csv\",\n",
    "               \"csv_results_40_255443_mp-06-parkstraat-2-la-filosovia.csv\",\n",
    "               \"csv_results_40_255444_mp-07-naamsestraat-81.csv\",\n",
    "               \"csv_results_40_255445_mp-08-kiosk-stadspark.csv\",\n",
    "               \"csv_results_40_280324_mp08bis---vrijthof.csv\",\n",
    "               \"csv_results_40_303910_mp-04-his-hears.csv\"]\n",
    "\n",
    "file_list_41 = [\"csv_results_41_255439_mp-01-naamsestraat-35-maxim.csv\",\n",
    "               \"csv_results_41_255440_mp-02-naamsestraat-57-xior.csv\",\n",
    "               \"csv_results_41_255441_mp-03-naamsestraat-62-taste.csv\",\n",
    "               \"csv_results_41_255442_mp-05-calvariekapel-ku-leuven.csv\",\n",
    "               \"csv_results_41_255443_mp-06-parkstraat-2-la-filosovia.csv\",\n",
    "               \"csv_results_41_255444_mp-07-naamsestraat-81.csv\",\n",
    "               \"csv_results_41_255445_mp-08-kiosk-stadspark.csv\",\n",
    "               \"csv_results_41_280324_mp08bis---vrijthof.csv\",\n",
    "               \"csv_results_41_303910_mp-04-his-hears.csv\"]\n",
    "\n",
    "file_list_42 = [\"csv_results_42_255439_mp-01-naamsestraat-35-maxim.csv\",\n",
    "               \"csv_results_42_255440_mp-02-naamsestraat-57-xior.csv\",\n",
    "               \"csv_results_42_255441_mp-03-naamsestraat-62-taste.csv\",\n",
    "               \"csv_results_42_255442_mp-05-calvariekapel-ku-leuven.csv\",\n",
    "               \"csv_results_42_255443_mp-06-parkstraat-2-la-filosovia.csv\",\n",
    "               \"csv_results_42_255444_mp-07-naamsestraat-81.csv\",\n",
    "               \"csv_results_42_255445_mp-08-kiosk-stadspark.csv\",\n",
    "               \"csv_results_42_280324_mp08bis---vrijthof.csv\",\n",
    "               \"csv_results_42_303910_mp-04-his-hears.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of files, takes a while\n",
    "file40 = merge_csv_files(folder_path + \"/export_40/\",file_list_40)\n",
    "file41 = merge_csv_files(folder_path + \"/export_41/\",file_list_41)\n",
    "file42 = merge_csv_files(folder_path + \"/export_42/\",file_list_42) #Uses the incomplete, reduced data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteo data\n",
    "\n",
    "file_list_meteo = [\"LC_2022Q1.csv\",\"LC_2022Q2.csv\",\"LC_2022Q3.csv\",\"LC_2022Q4.csv\",]\n",
    "\n",
    "# lots of files, takes a while\n",
    "meteo = merge_csv_files(folder_path + \"/meteodata/\",file_list_meteo,delim=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belgium_tz = pytz.timezone(\"Europe/Brussels\")\n",
    "\n",
    "# convert the 'dateutc' column to datetime objects and set the timezone to UTC\n",
    "meteo[\"DATEUTC\"] = pd.to_datetime(meteo[\"DATEUTC\"], utc=True)\n",
    "\n",
    "# localize the datetime objects to UTC and then convert to Belgium time\n",
    "meteo[\"datetime\"] = meteo[\"DATEUTC\"].apply(\n",
    "    lambda x: x.tz_convert(\"UTC\").astimezone(belgium_tz)\n",
    ")\n",
    "\n",
    "meteo.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the timezone information from the datetime objects\n",
    "meteo[\"datetime\"] = meteo[\"datetime\"].dt.tz_localize(None)\n",
    "meteo.to_csv(\"meteo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 41 - Noise event "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file41.groupby([\"#object_id\", \"description\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "file41.isna().mean()\n",
    "file41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unncessary cols\n",
    "cols_to_drop = [\n",
    "    \"noise_event_laeq_model_id_unit\",\n",
    "    \"noise_event_laeq_model_id\",\n",
    "    \"noise_event_laeq_primary_detected_certainty_unit\",\n",
    "    \"noise_event_laeq_primary_detected_class_unit\",\n",
    "]\n",
    "\n",
    "file41.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename cols\n",
    "file41.columns = [\n",
    "    \"object_id\",\n",
    "    \"location\",\n",
    "    \"result_timestamp\",\n",
    "    \"noise_event_certainty\",\n",
    "    \"noise_event\",\n",
    "]\n",
    "file41.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the timestamp col to datetime format\n",
    "file41[\"result_timestamp\"] = pd.to_datetime(file41[\"result_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file41.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from timestamp\n",
    "file41[\"time\"] = file41[\"result_timestamp\"].dt.time\n",
    "file41[\"date\"] = file41[\"result_timestamp\"].dt.date\n",
    "file41[\"hour\"] = file41[\"result_timestamp\"].dt.hour\n",
    "file41[\"weekday\"] = file41[\"result_timestamp\"].dt.strftime(\"%a\")\n",
    "weekday_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "file41[\"weekday\"] = pd.Categorical(\n",
    "    file41[\"weekday\"], categories=weekday_order, ordered=True\n",
    ")\n",
    "file41.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file41.to_csv(\"file41.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data by hour, weekday and calculate the count\n",
    "aggregated_df = (\n",
    "    file41.groupby([\"hour\", \"weekday\", \"noise_event\", \"location\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "aggregated_df\n",
    "# For each location\n",
    "locations = list(aggregated_df.location.unique())\n",
    "mp01 = aggregated_df[aggregated_df.location == locations[0]].drop([\"location\"], axis=1)\n",
    "weekday_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "mp01[\"weekday\"] = pd.Categorical(\n",
    "    mp01[\"weekday\"], categories=weekday_order, ordered=True\n",
    ")\n",
    "mp01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for only Transport sound\n",
    "mp01_car = mp01[mp01[\"noise_event\"] == \"Transport road - Passenger car\"].drop(\n",
    "    [\"noise_event\"], axis=1\n",
    ")\n",
    "mp01_car\n",
    "\n",
    "# Pivot the data to create a heatmap\n",
    "heatmap_data = mp01_car.pivot_table(\n",
    "    index=\"hour\", columns=\"weekday\", values=\"count\", fill_value=0\n",
    ")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, cmap=\"YlGnBu\", fmt=\"\", cbar=False)\n",
    "plt.title(\"Frequency of Transporting sound at MP01\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Hour\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file41 = pd.read_csv(\n",
    "    \"s3://teamchadmda/export_41/csv_results_41_255439_mp-01-naamsestraat-35-maxim.csv\", delimiter=\";\"\n",
    ")\n",
    "df_file41[\"result_timestamp\"] = pd.to_datetime(df_file41[\"result_timestamp\"])\n",
    "df_file41[\"date\"] = df_file41[\"result_timestamp\"].dt.date\n",
    "df_file41[\"hour\"] = df_file41[\"result_timestamp\"].dt.hour\n",
    "df_file41[\"weekday\"] = df_file41[\"result_timestamp\"].dt.strftime(\"%a\")\n",
    "cols_to_drop = [\n",
    "    \"noise_event_laeq_model_id_unit\",\n",
    "    \"noise_event_laeq_model_id\",\n",
    "    \"noise_event_laeq_primary_detected_certainty_unit\",\n",
    "    \"noise_event_laeq_primary_detected_class_unit\",\n",
    "    \"description\",\n",
    "    \"#object_id\",\n",
    "    \"noise_event_laeq_primary_detected_certainty\",\n",
    "]\n",
    "df_file41.drop(cols_to_drop, axis=1, inplace=True)\n",
    "df_file41.rename(\n",
    "    columns={\"noise_event_laeq_primary_detected_class\": \"noise_event\"}, inplace=True\n",
    ")\n",
    "df_file41.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_file41 = (\n",
    "    df_file41.groupby([\"hour\", \"weekday\", \"noise_event\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "weekday_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "aggregated_file41[\"weekday\"] = pd.Categorical(\n",
    "    aggregated_file41[\"weekday\"], categories=weekday_order, ordered=True\n",
    ")\n",
    "aggregated_file41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Transporation road - Passenger car` is by far the most common noise event. \n",
    "- We also notice more noise in `MP_01` location than in other locations, followed by `MP 07`\n",
    "- Only 7/9 locations appear in the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\")\n",
    "\n",
    "grouped = (\n",
    "    file41.groupby([\"location\", \"noise_event\"])[\"date\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=grouped,\n",
    "    x=\"noise_event\",\n",
    "    y=\"count\",\n",
    "    hue=\"location\",\n",
    "    kind=\"bar\",\n",
    "    height=6,\n",
    "    aspect=1.5,\n",
    ")\n",
    "\n",
    "g.set(\n",
    "    xlabel=\"Noise Event\",\n",
    "    ylabel=\"Frequency\",\n",
    "    title=\"Frequency by Location and Noise Event\",\n",
    ")\n",
    "plt.xticks(size=8)\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Transport road - Passenger car` occurs most frequently during 7am - 10am (when people start going to work). Interestingly, we don't see another peak during rush hour.\n",
    "- We see a peak of `Human voice - Shouting` during the midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by 'hour' and 'noise_event' and count occurrences\n",
    "grouped = (\n",
    "    file41.groupby([\"hour\", \"noise_event\"])[\"noise_event_certainty\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.lineplot(x=\"hour\", y=\"count\", hue=\"noise_event\", data=grouped)\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Frequency of Noise event by Hour\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `March - April` seems like the noisiest period\n",
    "- We see wide variation of noise event frequency during Spring\n",
    "- We see an unusual gap in the end of `January`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "\n",
    "# group data by 'date' and 'noise_event' and count occurrences\n",
    "grouped = (\n",
    "    file41.groupby([\"date\", \"noise_event\"])[\"noise_event_certainty\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.lineplot(x=\"date\", y=\"count\", hue=\"noise_event\", data=grouped)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Frequency of Noise event by Date\", size=15)\n",
    "\n",
    "# set xticks to show all months\n",
    "months = MonthLocator()\n",
    "date_format = DateFormatter(\"%b\")\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(date_format)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Thursday records the most noise events followed by Monday. But why?\n",
    " - Saturday and Sunday are the most quiet days, probably because students go home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "grouped = (\n",
    "    file41.groupby([\"weekday\", \"noise_event\"])[\"noise_event_certainty\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.lineplot(x=\"weekday\", y=\"count\", hue=\"noise_event\", data=grouped)\n",
    "ax.set_xlabel(\"Weekday\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Frequency of Noise event by Weekday\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 40 - Noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file40.tail(5)\n",
    "file40.to_csv(\"file40.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all _unit columns\n",
    "cols_to_drop = [col for col in file40.columns if col.endswith(\"unit\")]\n",
    "file40.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# rename columns\n",
    "file40.rename(columns={\"description\": \"location\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'result_timestamp' column to a datetime data type\n",
    "file40[\"result_timestamp\"] = pd.to_datetime(file40[\"result_timestamp\"])\n",
    "file40[\"date\"] = file40[\"result_timestamp\"].dt.date\n",
    "file40[\"hour\"] = file40[\"result_timestamp\"].dt.hour\n",
    "file40[\"weekday\"] = file40[\"result_timestamp\"].dt.strftime(\"%a\")\n",
    "file40.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The noise level usually peaks around 8-9 AM which coincides with the time people start going to work\n",
    "- The highest peaks are found during this hour in `MP 01`, this area also has highest frequency of Transportation sound. \n",
    "- If night sound should be below 40 dba for a good night sleep (according to WHO), then the area of `MP 03` and `MP 04` need to be regulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laf_cols = [col for col in file40.columns if col.startswith(\"laf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "locations = file40[\"location\"].unique()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 12), sharey=True)\n",
    "\n",
    "# loop over all location values and plot them in the grid\n",
    "for i, loc in enumerate(locations):\n",
    "\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "\n",
    "    # filter the data for the current location and group by hour\n",
    "    loc_data = file40[file40[\"location\"] == loc]\n",
    "    loc_hour = loc_data.groupby(\"hour\")[laf_cols].mean()\n",
    "\n",
    "    # create the line plot for each LAF column\n",
    "    for var in laf_cols:\n",
    "        sns.lineplot(data=loc_hour[var], label=None, ax=axs[row, col])\n",
    "\n",
    "    axs[row, col].set_title(f\"{loc}\")\n",
    "    axs[row, col].set_xlabel(\"Hour\")\n",
    "    axs[row, col].set_ylabel(\"dB(A)\")\n",
    "    axs[row, col].set_xticks(loc_hour.index)\n",
    "\n",
    "legend = fig.legend(\n",
    "    laf_cols, title=\"LAF values\", loc=\"lower right\", bbox_to_anchor=(1.1, 0.5)\n",
    ")\n",
    "\n",
    "# add a title to the whole plot\n",
    "fig.suptitle(\"Mean LAF by hour and location\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries that have maximum `laf005_per_hour` > 100 occures primarily on `July 6` and `July 25`. What kind of events happened on these days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file40[file40.laf005_per_hour > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "locations = file40[\"location\"].unique()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 12), sharey=True)\n",
    "\n",
    "# loop over all location values and plot them in the grid\n",
    "for i, loc in enumerate(locations):\n",
    "\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "\n",
    "    # filter the data for the current location and group by hour\n",
    "    loc_data = file40[file40[\"location\"] == loc]\n",
    "    loc_date = loc_data.groupby(\"date\")[laf_cols].mean()\n",
    "\n",
    "    # create the line plot for each LAF column\n",
    "    for var in laf_cols:\n",
    "        sns.lineplot(data=loc_date[var], label=None, ax=axs[row, col])\n",
    "\n",
    "    axs[row, col].set_title(f\"{loc}\")\n",
    "    axs[row, col].set_xlabel(\"Date\")\n",
    "    axs[row, col].set_ylabel(\"dB(A)\")\n",
    "\n",
    "legend = fig.legend(\n",
    "    laf_cols, title=\"LAF values\", loc=\"lower right\", bbox_to_anchor=(1.1, 0.5)\n",
    ")\n",
    "\n",
    "# add a title to the whole plot\n",
    "fig.suptitle(\"Mean LAF by date and location\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You usually see a peak during `July - September` in terms of max LAF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "locations = file40[\"location\"].unique()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 12), sharey=True)\n",
    "\n",
    "# loop over all location values and plot them in the grid\n",
    "for i, loc in enumerate(locations):\n",
    "\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "\n",
    "    # filter the data for the current location and group by hour\n",
    "    loc_data = file40[file40[\"location\"] == loc]\n",
    "    loc_date = loc_data.groupby(\"date\")[\n",
    "        \"laf05_per_hour\", \"laf50_per_hour\", \"laf95_per_hour\"\n",
    "    ].max()\n",
    "\n",
    "    # create the line plot for each LAF column\n",
    "    for var in [\"laf05_per_hour\", \"laf50_per_hour\", \"laf95_per_hour\"]:\n",
    "        sns.lineplot(data=loc_date[var], label=None, ax=axs[row, col])\n",
    "\n",
    "    axs[row, col].set_title(f\"{loc}\")\n",
    "    axs[row, col].set_xlabel(\"Date\")\n",
    "    axs[row, col].set_ylabel(\"dB(A)\")\n",
    "\n",
    "legend = fig.legend(\n",
    "    [\"laf05_per_hour\", \"laf50_per_hour\", \"laf95_per_hour\"],\n",
    "    title=\"LAF values\",\n",
    "    loc=\"lower right\",\n",
    "    bbox_to_anchor=(1.1, 0.5),\n",
    ")\n",
    "\n",
    "# add a title to the whole plot\n",
    "fig.suptitle(\"Max Laf values by date and location\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean LAF values usually peak on `Thu` and `Fri`. This coincides with our finding with File40 as we found transporation sound to occur most frequently on `Thu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "locations = file40[\"location\"].unique()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 12), sharey=True)\n",
    "\n",
    "# loop over all location values and plot them in the grid\n",
    "for i, loc in enumerate(locations):\n",
    "\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "\n",
    "    # filter the data for the current location and group by hour\n",
    "    loc_data = file40[file40[\"location\"] == loc]\n",
    "    loc_date = loc_data.groupby(\"weekday\")[\n",
    "        \"laf05_per_hour\", \"laf50_per_hour\", \"laf95_per_hour\"\n",
    "    ].mean()\n",
    "\n",
    "    # create the line plot for each LAF column\n",
    "    for var in [\"laf05_per_hour\", \"laf50_per_hour\", \"laf95_per_hour\"]:\n",
    "        sns.lineplot(data=loc_date[var], label=None, ax=axs[row, col])\n",
    "\n",
    "    axs[row, col].set_title(f\"{loc}\")\n",
    "    axs[row, col].set_xlabel(\"Weekday\")\n",
    "    axs[row, col].set_ylabel(\"dB(A)\")\n",
    "\n",
    "legend = fig.legend(\n",
    "    [\"laf05_per_hour\", \"laf50_per_hour\", \"laf95_per_hour\"],\n",
    "    title=\"LAF values\",\n",
    "    loc=\"lower right\",\n",
    "    bbox_to_anchor=(1.1, 0.5),\n",
    ")\n",
    "\n",
    "# add a title to the whole plot\n",
    "fig.suptitle(\"Mean Laf values by weekday and location\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample noise level by day in all locations\n",
    "laf_cols = [col for col in file40.columns if col.startswith(\"laf\")]\n",
    "noise_level_daily_mean = file40.copy()\n",
    "noise_level_daily_mean.rename(columns={\"result_timestamp\": \"datetime\"}, inplace=True)\n",
    "noise_level_daily_mean.set_index(\"datetime\", inplace=True)\n",
    "noise_level_daily_mean = noise_level_daily_mean[laf_cols].resample(\"D\").mean()\n",
    "noise_level_daily_mean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=noise_level_daily_mean)\n",
    "plt.legend(\n",
    "    loc=\"center left\", bbox_to_anchor=(1.0, 0.5), title=\"LAF values\", frameon=False\n",
    ")\n",
    "plt.title(\"Mean LAF by date in all locations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show max columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set time as index\n",
    "meteo.set_index(\"datetime\", inplace=True)\n",
    "# resample the data by day and take the mean\n",
    "lc_cols = [col for col in meteo.columns if col.startswith(\"LC\")]\n",
    "meteo_daily_mean = meteo[lc_cols].resample(\"D\").mean()\n",
    "meteo_hourly_mean = meteo[lc_cols].resample(\"H\").mean()\n",
    "meteo_daily_mean.reset_index(inplace=True)\n",
    "meteo_hourly_mean.reset_index(inplace=True)\n",
    "meteo_hourly_mean.to_csv(\"meteo_hourly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in lc_cols:\n",
    "    plt.figure()\n",
    "    sns.lineplot(data=meteo_daily_mean, x=\"datetime\", y=meteo_daily_mean[var])\n",
    "    plt.title(col)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between daily meteorological data and LAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level_daily_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge meteo daily data with mean daily noise level data\n",
    "\n",
    "\n",
    "noise_level_daily_mean.reset_index(inplace=True)\n",
    "meteo_noise_daily_mean = meteo_daily_mean.merge(\n",
    "    noise_level_daily_mean[[\"datetime\", \"laf50_per_hour\"]], on=[\"datetime\"]\n",
    ")\n",
    "meteo_noise_daily_mean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`laf50_per_hour` by date displays a week correlation with weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "palette = sns.diverging_palette(20, 220, n=256)\n",
    "corr = meteo_noise_daily_mean.corr(method=\"pearson\")\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=palette, center=0, annot_kws={\"size\": 8})\n",
    "plt.title(\n",
    "    \"Correlation Matrix between daily meteorological data and LAF value\",\n",
    "    size=15,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between hourly meteorological data and LAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the data by hour and take the mean\n",
    "lc_cols = [col for col in meteo.columns if col.startswith(\"LC\")]\n",
    "meteo_hourly_mean = meteo[lc_cols].resample(\"H\").mean()\n",
    "meteo_hourly_mean.reset_index(inplace=True)\n",
    "meteo_hourly_mean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample noise level by day in all locations\n",
    "laf_cols = [col for col in file40.columns if col.startswith(\"laf\")]\n",
    "noise_level_hourly_mean = file40.copy()\n",
    "noise_level_hourly_mean.rename(columns={\"result_timestamp\": \"datetime\"}, inplace=True)\n",
    "noise_level_hourly_mean.set_index(\"datetime\", inplace=True)\n",
    "noise_level_hourly_mean = noise_level_hourly_mean[laf_cols].resample(\"H\").mean()\n",
    "noise_level_hourly_mean.head(3)\n",
    "noise_level_hourly_mean.to_csv(\"file40_hourly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge meteo daily data with mean daily noise level data\n",
    "noise_level_hourly_mean.reset_index(inplace=True)\n",
    "meteo_noise_hourly_mean = meteo_hourly_mean.merge(\n",
    "    noise_level_hourly_mean[[\"datetime\", \"laf50_per_hour\"]], on=[\"datetime\"]\n",
    ")\n",
    "meteo_noise_hourly_mean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "palette = sns.diverging_palette(20, 220, n=256)\n",
    "corr = meteo_noise_hourly_mean.corr(method=\"pearson\")\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=palette, center=0, annot_kws={\"size\": 8})\n",
    "plt.title(\n",
    "    \"Correlation Matrix between hourly meteorological data and LAF value\",\n",
    "    size=15,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file41.to_csv(\"file41.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file41.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file41 = pd.read_csv(\"file41.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file41.result_timestamp = pd.to_datetime(file41.result_timestamp)\n",
    "file41.date = pd.to_datetime(file41.date)\n",
    "file41.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_file41 = (\n",
    "    file41.groupby([\"hour\", \"weekday\", \"date\", \"noise_event\", \"location\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "weekday_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "aggregated_file41[\"weekday\"] = pd.Categorical(\n",
    "    aggregated_file41[\"weekday\"], categories=weekday_order, ordered=True\n",
    ")\n",
    "aggregated_file41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = aggregated_file41.pivot_table(\n",
    "    index=\"location\", columns=\"weekday\", values=\"count\", fill_value=0\n",
    ")\n",
    "heatmap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap using Plotly\n",
    "fig = px.imshow(\n",
    "    heatmap_data.values,\n",
    "    x=heatmap_data.columns,\n",
    "    y=heatmap_data.index,\n",
    "    labels=dict(x=\"Weekday\", y=\"Location\", color=\"Frequency\"),\n",
    "    color_continuous_scale=\"YlOrRd\",\n",
    ")\n",
    "\n",
    "# Set the title\n",
    "fig.update_layout(title=\"Frequency of Noise Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data\n",
    "grouped = file41.groupby([\"location\", \"noise_event\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Create the bar plot using Plotly\n",
    "fig = px.bar(\n",
    "    grouped,\n",
    "    x=\"noise_event\",\n",
    "    y=\"count\",\n",
    "    color=\"location\",\n",
    "    title=\"Frequency by Location and Noise Event\",\n",
    "    labels={\"noise_event\": \"Noise Event\", \"count\": \"Frequency\"},\n",
    "    height=600,\n",
    "    width=900,\n",
    ")\n",
    "\n",
    "# Customize the x-axis tick labels\n",
    "fig.update_layout(xaxis={\"categoryorder\": \"total descending\"})\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses the incomplete, reduced version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file42.result_timestamp = pd.to_datetime(file42.result_timestamp)\n",
    "file42[\"date\"] = file42[\"result_timestamp\"].dt.date\n",
    "file42[\"month\"] = file42[\"result_timestamp\"].dt.month\n",
    "file42[\"hour\"] = file42[\"result_timestamp\"].dt.hour\n",
    "file42[\"weekday\"] = file42[\"result_timestamp\"].dt.strftime(\"%a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file42.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure weekday and lamax columns are of correct type\n",
    "file42[\"weekday\"] = pd.Categorical(\n",
    "    file42[\"weekday\"],\n",
    "    categories=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "    ordered=True,\n",
    ")\n",
    "file42[\"lamax\"] = file42[\"lamax\"].astype(float)\n",
    "\n",
    "# Define a color for each day of the week\n",
    "colors = sns.color_palette(\"husl\", 7)  # 'husl' color palette with 7 colors\n",
    "\n",
    "# Create a list of traces for each weekday\n",
    "traces = []\n",
    "for i, day in enumerate(file42[\"weekday\"].cat.categories):\n",
    "    color = \"rgb\" + str(\n",
    "        tuple(int(c * 255) for c in colors[i])\n",
    "    )  # Convert color to rgb format\n",
    "    traces.append(\n",
    "        go.Violin(\n",
    "            x=file42[\"lamax\"][file42[\"weekday\"] == day], line_color=color, name=day\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Define the layout\n",
    "layout = go.Layout(\n",
    "    title=\"Distribution of Lamax by Weekday\",\n",
    "    xaxis_title=\"Lamax\",\n",
    "    yaxis_title=\"Weekday\",\n",
    "    violingap=0,\n",
    "    violingroupgap=0,\n",
    "    violinmode=\"overlay\",\n",
    ")\n",
    "\n",
    "# Create the figure and add traces\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_violin_plot(df, groupby_col, title):\n",
    "    # Define a color palette\n",
    "    num_unique_values = df[groupby_col].nunique()\n",
    "    colors = sns.color_palette(\"husl\", num_unique_values)\n",
    "\n",
    "    # Create a list of traces for each unique value in the groupby column\n",
    "    traces = []\n",
    "    for i, val in enumerate(sorted(df[groupby_col].unique())):\n",
    "        color = \"rgb\" + str(\n",
    "            tuple(int(c * 255) for c in colors[i])\n",
    "        )  # Convert color to rgb format\n",
    "        traces.append(\n",
    "            go.Violin(\n",
    "                x=df[\"lamax\"][df[groupby_col] == val], line_color=color, name=str(val)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Define the layout\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Lamax\",\n",
    "        yaxis_title=groupby_col.capitalize(),\n",
    "        violingap=0,\n",
    "        violingroupgap=0,\n",
    "        violinmode=\"overlay\",\n",
    "    )\n",
    "\n",
    "    # Create the figure and add traces\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure 'hour' and 'lamax' columns are of correct type\n",
    "file42[\"hour\"] = file42[\"hour\"].astype(int)\n",
    "file42[\"lamax\"] = file42[\"lamax\"].astype(float)\n",
    "\n",
    "create_violin_plot(file42, \"hour\", \"Distribution of Lamax by Hour\")\n",
    "\n",
    "# Make sure 'month' and 'lamax' columns are of correct type\n",
    "file42[\"month\"] = file42[\"month\"].astype(int)\n",
    "\n",
    "create_violin_plot(file42, \"month\", \"Distribution of Lamax by Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files\n",
    "weather_data = pd.read_csv(\"../data/processed_weather_data_leuven.csv\", index_col=0)\n",
    "air_quality = pd.read_csv(\"../data/processed_air_quality_data.csv\", index_col=0)\n",
    "file42 = pd.read_csv(\"../data/processed_file42_data.csv\", index_col=0)\n",
    "# drop NaN\n",
    "file42.dropna(subset=\"lamax\", inplace=True)\n",
    "# rename time col\n",
    "file42.rename(columns={\"result_timestamp\": \"time\"}, inplace=True)\n",
    "air_quality.rename(columns={\"dt\": \"time\"}, inplace=True)\n",
    "# merge all df\n",
    "merged_df = pd.merge(\n",
    "    weather_data, air_quality, on=[\"time\", \"hour\", \"month\"], how=\"inner\"\n",
    ")\n",
    "merged_df = pd.merge(merged_df, file42, on=[\"time\", \"hour\", \"month\"], how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "palette = sns.diverging_palette(20, 220, n=256)\n",
    "corr = merged_df.corr(method=\"pearson\")\n",
    "sns.heatmap(corr, annot=False, fmt=\".2f\", cmap=palette, center=0, annot_kws={\"size\": 8})\n",
    "plt.title(\n",
    "    \"Correlation Matrix between daily meteorological data and noise measurements\",\n",
    "    size=15,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = merged_df.corr(method=\"pearson\")\n",
    "\n",
    "# Create heatmap\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=corr.values,\n",
    "        x=corr.columns,\n",
    "        y=corr.index,\n",
    "        colorscale=\"RdBu\",\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        colorbar=dict(title=\"Correlation\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"Correlation Matrix between daily meteorological data and noise measurements\",\n",
    "    width=900,\n",
    "    height=900,\n",
    "    xaxis=dict(title=\"Columns\"),\n",
    "    yaxis=dict(title=\"Rows\"),\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "pio.show(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
