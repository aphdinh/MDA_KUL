{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge data sets into one feature matrix\n",
    "\n",
    "air_data = pd.read_csv(\"data/processed_air_quality_data\")\n",
    "weather_data = pd.read_csv(\"data/processed_weather_data_leuven\")\n",
    "noise_file40 = pd.read_csv(\"data/processed_file40_data\")\n",
    "noise_file41 = pd.read_csv(\"data/processed_file41_data\")\n",
    "\n",
    "# Dropping unncessary index columns and location (object id = location)\n",
    "weather_data = weather_data.drop(\"Unnamed: 0\", axis=1)\n",
    "air_data = air_data.drop(\"Unnamed: 0\", axis=1)\n",
    "noise_file40 = noise_file40.drop([\"Unnamed: 0\", \"location\",\"result_timestamp\"], axis=1)\n",
    "noise_file41 = noise_file41. drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "full_df = weather_data.merge(air_data, on=[\"date\",\"hour\",\"month\",\"weekday\"])\n",
    "full_df = full_df.merge(noise_file40, on=[\"date\",\"hour\",\"month\",\"weekday\"])\n",
    "\n",
    "# Merge on object_id is important\n",
    "full_df = full_df.merge(noise_file41, on = [\"date\",\"hour\",\"month\",\"weekday\",\"object_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting into target_matrix and feature_matrix, train_set and test_set \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Selecting targets, dropping laf_features\n",
    "target_list = ['Human voice - Shouting','Human voice - Singing', 'Music non-amplified',\n",
    "       'Nature elements - Wind', 'Transport road - Passenger car', 'Transport road - Siren']\n",
    "laf_features = ['laf005_per_hour','laf01_per_hour', 'laf05_per_hour', 'laf10_per_hour', \n",
    "                 'laf25_per_hour','laf50_per_hour', 'laf75_per_hour', 'laf90_per_hour', \n",
    "                 'laf95_per_hour','laf98_per_hour', 'laf99_per_hour', 'laf995_per_hour']\n",
    "feature_list = [feature for feature in full_df.columns if feature not in target_list and feature not in laf_features]\n",
    "\n",
    "# Dropping date features\n",
    "date_features = [\"time\",\"date\",\"dt\"]\n",
    "for feature in date_features:\n",
    "    feature_list.remove(feature)\n",
    "\n",
    "# Splitting in train/test\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(full_df[feature_list], \n",
    "                                                                              full_df[target_list], test_size=0.2, random_state=1337)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in one_hot_var and numerical_var\n",
    "# Need to add winddirection transformation\n",
    "one_hot_var =[\"weathercode\",\"hour\",\"month\",\"weekday\",\"object_id\"]\n",
    "numerical_var = [col for col in train_features.columns if col not in one_hot_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('OneHot',\n",
       "                                 OneHotEncoder(handle_unknown='ignore'),\n",
       "                                 ['weathercode', 'hour', 'month', 'weekday',\n",
       "                                  'object_id']),\n",
       "                                ('StandardScaler', StandardScaler(),\n",
       "                                 ['temperature_2m', 'relativehumidity_2m',\n",
       "                                  'dewpoint_2m', 'apparent_temperature',\n",
       "                                  'pressure_msl', 'surface_pressure',\n",
       "                                  'precipitation', 'rain', 'snowfall',\n",
       "                                  'cloudcover', 'cloudcover_low',\n",
       "                                  'cloudcover_mid', 'cloudcover_high',\n",
       "                                  'shortwave_radiation', 'direct_radiation',\n",
       "                                  'diffuse_radiation',\n",
       "                                  'direct_normal_irradiance', 'windspeed_10m',\n",
       "                                  'winddirection_10m', 'windgusts_10m', 'pm2_5',\n",
       "                                  'pm10', 'co', 'no2', 'o3', 'so2', 'nh3'])])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "t = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('OneHot', OneHotEncoder(handle_unknown='ignore'), one_hot_var),\n",
    "        ('StandardScaler', StandardScaler(), numerical_var), \n",
    "        \n",
    "    ] )\n",
    "\n",
    "# fit the encoder\n",
    "t.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas DataFrame from dense matrix\n",
    "# To array because we may run into version troubles otherwise\n",
    "X_train = pd.DataFrame(t.transform(train_features), columns=t.get_feature_names_out())\n",
    "\n",
    "X_val = pd.DataFrame(t.transform(test_features), columns=t.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV for random_forest...\n",
      "Best parameters for random_forest:  {'bootstrap': False, 'max_depth': 31, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 85}\n",
      "Best score for random_forest:  0.13109480614845506\n",
      "\n",
      "\n",
      "Running RandomizedSearchCV for gradient_boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 494, in fit\n",
      "    y = column_or_1d(y, warn=True)\n",
      "  File \"c:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1024, in column_or_1d\n",
      "    raise ValueError(\n",
      "ValueError: y should be a 1d array, got an array of shape (12877, 6) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 494, in fit\n",
      "    y = column_or_1d(y, warn=True)\n",
      "  File \"c:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1024, in column_or_1d\n",
      "    raise ValueError(\n",
      "ValueError: y should be a 1d array, got an array of shape (12878, 6) instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (16097, 6) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m random_search \u001b[39m=\u001b[39m RandomizedSearchCV(model, param_distributions\u001b[39m=\u001b[39mparam_dist,\n\u001b[0;32m     58\u001b[0m                                    n_iter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[39m# Fit the RandomizedSearchCV object to the data\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m random_search\u001b[39m.\u001b[39;49mfit(X_train, train_targets)\n\u001b[0;32m     63\u001b[0m \u001b[39m# Print the best parameters and score\u001b[39;00m\n\u001b[0;32m     64\u001b[0m params_dict[model_name] \u001b[39m=\u001b[39m random_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    925\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 926\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    927\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    490\u001b[0m sample_weight_is_none \u001b[39m=\u001b[39m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    492\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n\u001b[1;32m--> 494\u001b[0m y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    496\u001b[0m \u001b[39mif\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    497\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_y(y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\svnel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1024\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m   1015\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1016\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1017\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected. Please change the shape of y to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   1021\u001b[0m         )\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mravel(y)\n\u001b[1;32m-> 1024\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1025\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[0;32m   1026\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (16097, 6) instead."
     ]
    }
   ],
   "source": [
    "# Throw old models at it because why not. \n",
    "# In theory for events poission regression seems nice too\n",
    "\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define the model parameters\n",
    "model_params = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': randint(50, 100),\n",
    "            'max_depth': randint(3, 50),\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': randint(50, 100),\n",
    "            'learning_rate': uniform(0.01, 0.5),\n",
    "            'max_depth': randint(1, 10),\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'xgboost': {\n",
    "        'model': xgboost.XGBRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': randint(50, 100),\n",
    "            'learning_rate': uniform(0.01, 0.5),\n",
    "            'max_depth': randint(1, 10),\n",
    "            'min_child_weight': randint(1, 10),\n",
    "            'gamma': uniform(0, 1),\n",
    "            'reg_alpha': uniform(0, 1),\n",
    "            'reg_lambda': uniform(0, 1),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "params_dict = {}\n",
    "\n",
    "# Loop through each model in model_params and run RandomizedSearchCV\n",
    "for model_name, model_info in model_params.items():\n",
    "    print(\"Running RandomizedSearchCV for {}...\".format(model_name))\n",
    "    \n",
    "    # Create a RandomizedSearchCV object for the current model\n",
    "    model = model_info['model']\n",
    "    param_dist = model_info['params']\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                       n_iter=10, cv=5, n_jobs=1, random_state=7)\n",
    "    \n",
    "    # Fit the RandomizedSearchCV object to the data\n",
    "    random_search.fit(X_train, train_targets)\n",
    "    \n",
    "    # Print the best parameters and score\n",
    "    params_dict[model_name] = random_search.best_params_\n",
    "    print(\"Best parameters for {}: \".format(model_name), random_search.best_params_)\n",
    "    print(\"Best score for {}: \".format(model_name), random_search.best_score_)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
